---
pretext: Library
title: Computing partial derivatives
subtitle: Dec 2022
inline: false
---

In the following article I'll write simple implementations of forwards mode and reverse mode automatic differentiation in Rust. Unfortunately, this is an *extremely* large topic and I can't cover everything in a 5 minute blog post. I'll try to keep it as simple as possible, but I might gloss over some finer points.

Automatic differentiation is a technique to compute the derivative of a function. It is used in machine learning to compute the gradient of a loss function w.r.t. the parameters of a model. This is used, among other things, to optimize the model.

===

===

# Forwards mode: Dual Numbers

We can use a system called dual numbers which allows us to compute a function and its derivative at the same time.

Dual numbers works as following: we have a type `T` which we can add, multiply, etc. We then define a new type `Dual<T>` which is a tuple of two `T`s.

<br/>
<m>z = a + bε</m>
<br/>

Note that: 

<br/>
<m>ε^2 = 0</m>
<br/>

It follows that:
<br/>
<m> (a+bε)(c+dε)=ac+(ad+bc)ε</m>

<br/>
<br/>

## Dual number arithmatic

When we add dual numbers, we add the real parts and the dual parts. When we multiply dual numbers, we use the product rule to compute the derivative of the dual part. We use the quotient rule for division.

To differentiate w.r.t. a variable, we set the dual part to 1. Every other variable gets set to 0 to indicate that they're constants.

Unfortunately, dual numbers are not very efficient for computing derivatives of functions with many inputs. This is because, for each input we need to set the dual part to 1 (i.e. computing w.r.t. said input) and take another forward pass.

However, most automatic differentiation is not done in the context of billions of weights, but rather optimizing over a small number of parameters.

===

```rust
// Note that `.wrt()` is called
// before we start defining our function.

let x = Value::new(2f64).wrt();
let y = Value::new(3f64);

let a = x * y;

println!("Value: ", a.real());
println!("Grad: ", a.grad());

```

===

## Implementation

Let's implement this as a generic struct `Value<T>` which can be used for any type `T` which implements `Clone` + `Copy` + `NumOps` + `Zero` + `One`.

===

```rust

pub struct Value
  <T: Clone + Copy + NumOps + Zero + One> {
  real: T,
  dual: T,
}

```

===

We need to implement `Zero` and `One` because we need to be able to set the dual part to 0 and 1 to signify which input we are computing the derivative w.r.t.

===

```rust
pub fn new(real: T) -> Self {
  Self {
    real,
    dual: T::zero(),
  }
}

pub fn wrt(self) -> Self {
  Self {
    real: self.real,
    dual: T::one(),
  }
}
```

===

We can now implement the arithmatic operators for `Value<T>`.

- As you can see, we add the real parts and the dual parts for the addition operator
- For the multiplication operator, we need to use the product rule to compute the derivative
- Likewise the division operator uses the quotient rule.

===

```rust
impl<T: IsDual> Add for Value<T> {
  type Output = Self;

  fn add(self, other: Self) -> Self {
    Self {
      real: self.real + other.real,
      dual: self.dual + other.dual,
    }
  }
}

impl<T: IsDual> Mul for Value<T> {
  type Output = Self;

  fn mul(self, other: Self) -> Self {
    let product = self.real * other.real;

    let vdu = self.real * other.dual;
    let udv = self.dual * other.real;
    Self {
      real: product,
      dual: vdu + udv,
    }
  }
}

```

===

# Reverse-mode: Tapes 

Writing a reverse-mode automatic differentiation system is pretty tricky in Rust. Most of the time, you end up fighting the borrow checker which really doesn't like recursive types.

I read an excellent blog post: [Reverse-mode automatic differentiation: a tutorial](https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation), explaining how to implement reverse-mode automatic differentiation in Python. It uses a tape based approach to compute the derivatives. The tape is a list of operations which are applied to the input variables to compute the output. The tape is then traversed backwards to compute the derivatives.

<br/>

> Conceptually, it looks like this: 
> ![tape](https://rufflewind.com/img/reverse-mode-automatic-differentiation-links.png)

<br/>

<cite>(Thank you, Rufflewind!)</cite>

## Implementation

I wanted to implement this generically, allowing users to define their own types given they implement traits such as `NumOps` + `Zero` + `One`. This could lead to the use of some interesting primitive types (complex numbers, normal distributions, quaternions, anyone?)

Behind the scenes, when you use an operator such as `+` or `*`, we create a new `Node` and push it onto the tape. The `Node` contains the location on the tape of both parents. It also contains fields for the partial derivative of each branch of the `Node`.

Each value contains the index of the tape position where the `Node` is stored. This allows us to `grad.wrt(x)` to get the partial derivative of the output w.r.t. the x.

On the backwards pass, we accumulate the derivatives of the output w.r.t. the inputs. We do this by traversing the tape backwards and using the chain rule to compute the derivatives.

We can introduce constants by pushing a new node onto the tape with the value of the constant and setting the partial derivative of the constant to 1.

===

```rust
// Note that `.wrt()` is called
// after we define our function

let scope = Scope::new();
let a = scope.value(7.0);
let b = scope.value(11.0);
let c = scope.value(13.0);

let z = a * b + c;
let grad = z.backwards();

println!("dz/dx = {}", grad.wrt(a));
println!("dz/dy = {}", grad.wrt(b));
println!("dz/dy = {}", grad.wrt(c));
```

===

It's a pretty brief introduction, but I hope it's enough to put across my point.

You kind find the source code for the reverse-mode autodiff on my [Github](https://github.com/lewis-carson). It's still a work in progress, but it's already pretty usable. In the future, I'd like to expand this into a full machine learning toolkit.

If you noticed any mistakes in this article or have any suggestions, please let me know by [emailing me](mailto:lewiscampbellcarson@gmail.com).

===